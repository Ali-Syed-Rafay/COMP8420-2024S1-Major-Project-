{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e79ff33-fb06-471d-ad74-5c39eb9684be",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">COMP8420 ADV NLP FINAL PROJECT</h2>\n",
    "<h2 align=\"center\">MultiLingAI: Multilingual Contextual Summarization for Global Enterprises</h2>\n",
    "\n",
    "<h2 align=\"center\">Submitted by:<h3>\n",
    "<h4 align=\"center\">Muhammad Haris Rizwan | Student ID: 47565284 </h4>\n",
    "<h4 align=\"center\">Syed Rafay Ali | Student ID: 47833920 </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3598fb6-c384-4599-9edb-cbf3813bc74b",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "1. [Introduction](#1.-Introduction)\n",
    "2. [Dataset](#2.-Dataset)\n",
    "3. [Data Preprocessing](#3.-Data-Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a020fe-6cff-44e4-a134-4fabff121b3d",
   "metadata": {},
   "source": [
    "# __1. Introduction__\n",
    "\n",
    "![MULTILINGAI](images/MULTILINGAI_PIC.jpeg)\n",
    "\n",
    "In this project, we assume the role of engineers at `MultiLinguaAI`, an IT company specializing in advanced Natural Language Processing (NLP) solutions for global enterprises. `MultiLinguaAI` offers a variety of services, including sentiment analysis, text summarization, named entity recognition, and chatbots. Our primary task is to develop and implement a multilingual summarization tool that addresses the unique challenges faced by these enterprises.\n",
    "\n",
    "## __Problem Statement__\n",
    "Global enterprises operate across multiple regions and languages, requiring accurate and context-preserving summaries of documents in various languages. This need is driven by the necessity to streamline operations, enhance communication, and ensure that vital information is accessible and understandable to all stakeholders, regardless of their linguistic background.\n",
    "\n",
    "## __Objective__\n",
    "The objective of our project is to develop a multilingual summarization tool that can generate accurate and contextually relevant summaries for documents written in multiple languages. This tool aims to maintain the integrity and key information of the original documents while making them concise and easy to understand for a diverse global audience.\n",
    "\n",
    "## __Project Scope__\n",
    "The scope of our project involves addressing the real-world challenge of handling and summarizing large volumes of multilingual documents.\n",
    "* Our target users are global enterprises with diverse linguistic documentation needs. \n",
    "* By leveraging advanced NLP models such as mBERT, XLM-R, and multilingual T5, we aim to create a robust solution that can be seamlessly integrated into the company's existing systems.\n",
    "* The project will include data collection, preprocessing, model training, evaluation, and integration phases, ensuring a comprehensive approach to solving this complex problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bb220-a401-452c-b868-44c034114ee8",
   "metadata": {},
   "source": [
    "# __2. Dataset__\n",
    "\n",
    "![dataset](images/dataset_pic.webp)\n",
    "\n",
    "For our project on Multilingual Contextual Summarization for Global Enterprises, the dataset plays a critical role in ensuring the accuracy and relevance of the generated summaries. We have selected datasets that provide a diverse and comprehensive collection of multilingual documents, which are essential for training and evaluating our models.\n",
    "\n",
    "## __Selected Dataset__\n",
    "We will utilize the MLSUM dataset, which stands out as a large-scale multilingual summarization dataset. MLSUM contains over 1.5 million article-summary pairs in five different languages: French, German, Spanish, Russian, and Turkish. This dataset is particularly suitable for our project because it offers a wide variety of articles and summaries from reputable news sources, ensuring both the quality and diversity needed for robust model training.\n",
    "\n",
    "## __References:__\n",
    "* `MLSUM`: The Multilingual Summarization Corpus - This dataset was introduced to facilitate research in multilingual text summarization by providing a large-scale, diverse set of news articles and summaries. It includes articles from five languages and aims to enable new research directions in the text summarization community. Link to paper​​.\n",
    "\n",
    "* `XL-Sum`: Large-Scale Multilingual Abstractive Summarization - XL-Sum provides an extensive collection of multilingual summarization data, enhancing the ability to develop models that perform well across various languages. This dataset complements MLSUM by offering additional resources and benchmarks for evaluating summarization models. Link to paper​​.\n",
    "\n",
    "* Contrastive Aligned Joint Learning for Multilingual Summarization - This reference explores novel methods for improving multilingual summarization, focusing on contrastive learning strategies. It provides insights into the challenges and solutions for developing high-quality summarization models, which will be valuable for refining our approach. Link to paper​​.\n",
    "\n",
    "## __Selected Dataset Details__\n",
    "\n",
    "### __MLSUM:__\n",
    "\n",
    "* Contents: Contains over 1.5 million article-summary pairs from five languages.\n",
    "* Languages: French, German, Spanish, Russian, Turkish.\n",
    "* Source: News articles from reputable sources.\n",
    "* Data Collection Process: We will collect the dataset from public repositories and ensure it is preprocessed for tokenization, normalization, and language detection. This preprocessing step is crucial for preparing the data for model training.\n",
    "### __CNN/DailyMail:__\n",
    "\n",
    "* Contents: Contains over 300,000 article-summary pairs.\n",
    "* Languages: English.\n",
    "* Source: News articles primarily from CNN and the Daily Mail, providing a rich source of diverse topics and high-quality journalism.\n",
    "* Data Collection Process: The dataset is available through Hugging Face and will be directly accessed using the datasets library. It includes preprocessing steps such as tokenization and normalization. The dataset is structured into three splits: train, validation, and test, facilitating the training and evaluation of summarization models. The article column contains the full text, while the highlights column contains the summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9761f76-b09b-476f-874f-55299a4dfb61",
   "metadata": {},
   "source": [
    "# __3. Data Preprocessing__\n",
    "\n",
    "![Process flow](images/process_flow_pic.webp)\n",
    "* __Data Visualisation__: Loading and opening the dataset(s) to see what is in it and what can be done with it.\n",
    "* __Data cleaning__: Removing unnecessary observations for the sake of project scope e.g. Removing extra columns, punctuations, and lowercase the text.\n",
    "* __Normalization__: Standardizing text data to remove inconsistencies.\n",
    "* __Tokenization__: Splitting text into words or subwords to facilitate model understanding.\n",
    "* __Language Detection__: Identifying and labeling the language of each document to ensure accurate processing.\n",
    "By leveraging the MLSUM dataset and incorporating insights from the referenced works, we aim to develop a robust multilingual summarization tool that meets the needs of global enterprises, providing accurate and context-preserving summaries across multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54c34ab-ead9-4374-9bc7-3e40b213eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rafay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/rafay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-06-08 02:10:04.920947: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Relevant libraries\n",
    "\n",
    "# dataset\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# model \n",
    "from transformers import MBart50Tokenizer, MBartForConditionalGeneration, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b642e3db-fc09-4bc4-a9b7-e2409fb0a3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.11.3 (main, Apr 19 2023, 18:51:09) [Clang 14.0.6 ]\n",
      "Version info.\n",
      "sys.version_info(major=3, minor=11, micro=3, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print(sys.version)\n",
    "print(\"Version info.\")\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90ad2b-0690-4bb1-95ee-898338be8188",
   "metadata": {},
   "source": [
    "## STEP 1: Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891b0471-c530-40ae-b11d-3c985aa5269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafay/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "#Load MLSUM dataset for French\n",
    "dataset_fr = load_dataset(\"mlsum\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6163f98f-b688-4ab2-9a0a-969d3f74748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MLSUM dataset for German\n",
    "dataset_de = load_dataset(\"mlsum\", \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659a54b8-5ade-4e3a-abe8-847dd20f8304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CNN/DailyMail dataset for english\n",
    "dataset_eng = load_dataset('cnn_dailymail', '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96ed3370-b80b-472b-9cec-522270f41054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET-FRENCH DETAILS: {'train': (392902, 6), 'validation': (16059, 6), 'test': (15828, 6)}\n",
      "DATASET-GERMAN DETAILS: {'train': (220887, 6), 'validation': (11394, 6), 'test': (10701, 6)}\n",
      "DATASET-ENGLISH DETAILS: {'train': (287113, 3), 'validation': (13368, 3), 'test': (11490, 3)}\n"
     ]
    }
   ],
   "source": [
    "# Print dataset details\n",
    "print(\"DATASET-FRENCH DETAILS:\",dataset_fr.shape)\n",
    "print(\"DATASET-GERMAN DETAILS:\", dataset_de.shape)\n",
    "print(\"DATASET-ENGLISH DETAILS:\",dataset_eng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029dc565-002e-4d24-b2b9-381c1d8a771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bifercating the datsets (splits)\n",
    "train_fr = dataset_fr['train']\n",
    "validation_fr = dataset_fr['validation']\n",
    "test_fr = dataset_fr['test']\n",
    "\n",
    "train_de = dataset_de['train']\n",
    "validation_de = dataset_de['validation']\n",
    "test_de = dataset_de['test']\n",
    "\n",
    "train_eng = dataset_eng['train']\n",
    "validation_eng = dataset_eng['validation']\n",
    "test_eng = dataset_eng['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b66bbe35-05e9-46e0-a701-a7b52d4e8777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "train_df_fr = pd.DataFrame(train_fr)\n",
    "validation_df_fr = pd.DataFrame(validation_fr)\n",
    "test_df_fr = pd.DataFrame(test_fr)\n",
    "\n",
    "train_df_de = pd.DataFrame(train_de)\n",
    "validation_df_de = pd.DataFrame(validation_de)\n",
    "test_df_de = pd.DataFrame(test_de)\n",
    "\n",
    "train_df_eng = pd.DataFrame(train_eng)\n",
    "validation_df_eng = pd.DataFrame(validation_eng)\n",
    "test_df_eng = pd.DataFrame(test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8111881-f75b-4254-8806-34b3e69cb15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "      <td>livres</td>\n",
       "      <td>https://www.lemonde.fr/livres/article/2010/01/...</td>\n",
       "      <td>La rentrée littéraire promet un programme de b...</td>\n",
       "      <td>01/01/2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Une semaine après l'attaque terroriste manquée...</td>\n",
       "      <td>Cette demande intervient une semaine après l'a...</td>\n",
       "      <td>proche-orient</td>\n",
       "      <td>https://www.lemonde.fr/proche-orient/article/2...</td>\n",
       "      <td>Gordon Brown appelle à une réunion internation...</td>\n",
       "      <td>01/01/2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...   \n",
       "1  Une semaine après l'attaque terroriste manquée...   \n",
       "\n",
       "                                             summary          topic  \\\n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...         livres   \n",
       "1  Cette demande intervient une semaine après l'a...  proche-orient   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.lemonde.fr/livres/article/2010/01/...   \n",
       "1  https://www.lemonde.fr/proche-orient/article/2...   \n",
       "\n",
       "                                               title        date  \n",
       "0  La rentrée littéraire promet un programme de b...  01/01/2010  \n",
       "1  Gordon Brown appelle à une réunion internation...  01/01/2010  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_fr.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08f3e8ed-5fc4-4830-8773-230602922932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79051</th>\n",
       "      <td>Unter den Internetunternehmen ist Twitter eine...</td>\n",
       "      <td>Unter den Internetunternehmen ist Twitter eine...</td>\n",
       "      <td>wirtschaft</td>\n",
       "      <td>https://www.sueddeutsche.de/wirtschaft/twitter...</td>\n",
       "      <td>Twitter-Börsengang: Reich durch Zwitschern</td>\n",
       "      <td>00/10/2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71089</th>\n",
       "      <td>Nach einem Sabbatjahr kehrt Ronnie O'Sullivan ...</td>\n",
       "      <td>Nach einem Sabbatjahr kehrt Ronnie O'Sullivan ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>https://www.sueddeutsche.de/sport/snooker-welt...</td>\n",
       "      <td>Snooker-Weltmeister Ronnie O'Sullivan - Erholt...</td>\n",
       "      <td>00/05/2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "79051  Unter den Internetunternehmen ist Twitter eine...   \n",
       "71089  Nach einem Sabbatjahr kehrt Ronnie O'Sullivan ...   \n",
       "\n",
       "                                                 summary       topic  \\\n",
       "79051  Unter den Internetunternehmen ist Twitter eine...  wirtschaft   \n",
       "71089  Nach einem Sabbatjahr kehrt Ronnie O'Sullivan ...       sport   \n",
       "\n",
       "                                                     url  \\\n",
       "79051  https://www.sueddeutsche.de/wirtschaft/twitter...   \n",
       "71089  https://www.sueddeutsche.de/sport/snooker-welt...   \n",
       "\n",
       "                                                   title        date  \n",
       "79051         Twitter-Börsengang: Reich durch Zwitschern  00/10/2013  \n",
       "71089  Snooker-Weltmeister Ronnie O'Sullivan - Erholt...  00/05/2013  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_de.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e865750a-9a6c-4a9f-a7e1-f218995d0304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "      <td>42c027e4ff9730fbb3de84c1af0d2c506e41c3e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "      <td>ee8871b15c50d0db17b0179a6d2beab35065f1e9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1  Editor's note: In our Behind the Scenes series...   \n",
       "\n",
       "                                          highlights  \\\n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...   \n",
       "1  Mentally ill inmates in Miami are housed on th...   \n",
       "\n",
       "                                         id  \n",
       "0  42c027e4ff9730fbb3de84c1af0d2c506e41c3e4  \n",
       "1  ee8871b15c50d0db17b0179a6d2beab35065f1e9  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_eng.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fae2df2-d439-4eb8-b356-ec8addd8cf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3788</th>\n",
       "      <td>After West Ham announced a vast reduction in s...</td>\n",
       "      <td>West Ham became first Premier League club to d...</td>\n",
       "      <td>fd984802497ba123291ca39b1f763d3ae195d831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6292</th>\n",
       "      <td>Hassan Munshi, one of two teenagers feared to ...</td>\n",
       "      <td>Families of Hassan Munshi and Talha Asmal, bot...</td>\n",
       "      <td>b410ef51a9d6c9b4566b2d69b02e877500f07357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  \\\n",
       "3788  After West Ham announced a vast reduction in s...   \n",
       "6292  Hassan Munshi, one of two teenagers feared to ...   \n",
       "\n",
       "                                             highlights  \\\n",
       "3788  West Ham became first Premier League club to d...   \n",
       "6292  Families of Hassan Munshi and Talha Asmal, bot...   \n",
       "\n",
       "                                            id  \n",
       "3788  fd984802497ba123291ca39b1f763d3ae195d831  \n",
       "6292  b410ef51a9d6c9b4566b2d69b02e877500f07357  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_eng.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b2870e-28b2-439c-b882-49c67a797421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.7 s, sys: 1.51 s, total: 10.2 s\n",
      "Wall time: 15.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# only want the text/article and summary/highlights columns from the three datasets (French, German, English) for now\n",
    "# Convert to Pandas DataFrame and select only the required columns (French)\n",
    "train_df_fr1 = pd.DataFrame(train_fr)[['text', 'summary']]\n",
    "validation_df_fr1 = pd.DataFrame(validation_fr)[['text', 'summary']]\n",
    "test_df_fr1 = pd.DataFrame(test_fr)[['text', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec9a93d-5691-4f18-883f-154daa42555a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 707 ms, total: 5.31 s\n",
      "Wall time: 8.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Convert to Pandas DataFrame and select only the required columns (German)\n",
    "train_df_de1 = pd.DataFrame(train_de)[['text', 'summary']]\n",
    "validation_df_de1 = pd.DataFrame(validation_de)[['text', 'summary']]\n",
    "test_df_de1 = pd.DataFrame(test_de)[['text', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb053032-d262-44ee-9e4c-ed92d2adb158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.62 s, sys: 1.19 s, total: 4.81 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Convert to Pandas DataFrame and select only the required columns (English)\n",
    "train_df_eng1 = pd.DataFrame(train_eng)[['article', 'highlights']]\n",
    "validation_df_eng1 = pd.DataFrame(validation_eng)[['article', 'highlights']]\n",
    "test_df_eng1 = pd.DataFrame(test_eng)[['article', 'highlights']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "940f3285-1ae6-4c8f-bfe4-b56869f98f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "      <td>Jean-Jacques Schuhl, Gilles Leroy, Christian G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Une semaine après l'attaque terroriste manquée...</td>\n",
       "      <td>Cette demande intervient une semaine après l'a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...   \n",
       "1  Une semaine après l'attaque terroriste manquée...   \n",
       "\n",
       "                                             summary  \n",
       "0  Jean-Jacques Schuhl, Gilles Leroy, Christian G...  \n",
       "1  Cette demande intervient une semaine après l'a...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the resulting DataFrames to verify the columns\n",
    "train_df_fr1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bc00b39-f25c-463f-8545-0660bc8e41bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220885</th>\n",
       "      <td>In Deutschland gibt es ihn bisher vor allem an...</td>\n",
       "      <td>Explosiv und höllisch stark: Das chinesische N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220886</th>\n",
       "      <td>Der Weihnachtsbaum vor dem Reichstag ist eine ...</td>\n",
       "      <td>Die Deutschen lieben ihre Weihnachtsbäume. Abe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "220885  In Deutschland gibt es ihn bisher vor allem an...   \n",
       "220886  Der Weihnachtsbaum vor dem Reichstag ist eine ...   \n",
       "\n",
       "                                                  summary  \n",
       "220885  Explosiv und höllisch stark: Das chinesische N...  \n",
       "220886  Die Deutschen lieben ihre Weihnachtsbäume. Abe...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_de1.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "903356a0-dcff-4fe2-83f0-0e0baa5000f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series...</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  LONDON, England (Reuters) -- Harry Potter star...   \n",
       "1  Editor's note: In our Behind the Scenes series...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Harry Potter star Daniel Radcliffe gets £20M f...  \n",
       "1  Mentally ill inmates in Miami are housed on th...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_eng1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d89ad-8d84-4a09-b249-3ba7354b9f71",
   "metadata": {},
   "source": [
    "## STEP 2: Data Cleaning: Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b58711f6-259a-4f2c-9cea-d2a9e1e05c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def preprocess_dataset(df, text_column, summary_column):\n",
    "    df[text_column] = df[text_column].apply(preprocess_text)\n",
    "    df[summary_column] = df[summary_column].apply(preprocess_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2276d828-30c0-430a-a549-534d9f0f2cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.5 s, sys: 5.85 s, total: 57.3 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess the French dataset\n",
    "train_df_fr_processed = preprocess_dataset(train_df_fr1, 'text', 'summary')\n",
    "validation_df_fr_processed = preprocess_dataset(validation_df_fr1, 'text', 'summary')\n",
    "test_df_fr_processed = preprocess_dataset(test_df_fr1, 'text', 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67dd180e-8b87-4222-aab6-8dfc95110e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.3 s, sys: 1.84 s, total: 33.1 s\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess the German dataset\n",
    "train_df_de_processed = preprocess_dataset(train_df_de1, 'text', 'summary')\n",
    "validation_df_de_processed = preprocess_dataset(validation_df_de1, 'text', 'summary')\n",
    "test_df_de_processed = preprocess_dataset(test_df_de1, 'text', 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e019592-15da-4fdd-a8c3-65e34c3faeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.8 s, sys: 3.77 s, total: 34.5 s\n",
      "Wall time: 53.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess the English dataset\n",
    "train_df_eng_processed = preprocess_dataset(train_df_eng1, 'article', 'highlights')\n",
    "validation_df_eng_processed = preprocess_dataset(validation_df_eng1, 'article', 'highlights')\n",
    "test_df_eng_processed = preprocess_dataset(test_df_eng1, 'article', 'highlights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c3b385d-125a-46a9-8d8f-de6aecfaf54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jeanjacques schuhl gilles leroy christian gail...</td>\n",
       "      <td>jeanjacques schuhl gilles leroy christian gail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>une semaine après lattaque terroriste manquée ...</td>\n",
       "      <td>cette demande intervient une semaine après lat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  jeanjacques schuhl gilles leroy christian gail...   \n",
       "1  une semaine après lattaque terroriste manquée ...   \n",
       "\n",
       "                                             summary  \n",
       "0  jeanjacques schuhl gilles leroy christian gail...  \n",
       "1  cette demande intervient une semaine après lat...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the preprocessed French DataFrame\n",
    "train_df_fr_processed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71f5d71b-de1e-4f26-ad47-f7c6eea4f297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220885</th>\n",
       "      <td>in deutschland gibt es ihn bisher vor allem an...</td>\n",
       "      <td>explosiv und höllisch stark das chinesische na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220886</th>\n",
       "      <td>der weihnachtsbaum vor dem reichstag ist eine ...</td>\n",
       "      <td>die deutschen lieben ihre weihnachtsbäume aber...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "220885  in deutschland gibt es ihn bisher vor allem an...   \n",
       "220886  der weihnachtsbaum vor dem reichstag ist eine ...   \n",
       "\n",
       "                                                  summary  \n",
       "220885  explosiv und höllisch stark das chinesische na...  \n",
       "220886  die deutschen lieben ihre weihnachtsbäume aber...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the preprocessed German DataFrame\n",
    "train_df_de_processed.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15d755d9-fb58-4983-90f0-c27a0e73263e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>london england reuters  harry potter star dani...</td>\n",
       "      <td>harry potter star daniel radcliffe gets £20m f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>editors note in our behind the scenes series c...</td>\n",
       "      <td>mentally ill inmates in miami are housed on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  london england reuters  harry potter star dani...   \n",
       "1  editors note in our behind the scenes series c...   \n",
       "\n",
       "                                          highlights  \n",
       "0  harry potter star daniel radcliffe gets £20m f...  \n",
       "1  mentally ill inmates in miami are housed on th...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the preprocessed English DataFrame\n",
    "train_df_eng_processed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "489a26bc-dbc3-44a0-91db-26a490e976e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sake of each, we will rename the English dataset's columns to the ones of the other two as follows:\n",
    "# Rename the columns in the English dataset\n",
    "train_df_eng_processed.rename(columns={'article': 'text', 'highlights': 'summary'}, inplace=True)\n",
    "validation_df_eng_processed.rename(columns={'article': 'text', 'highlights': 'summary'}, inplace=True)\n",
    "test_df_eng_processed.rename(columns={'article': 'text', 'highlights': 'summary'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd764225-69f5-4dfa-ad24-8b153d313d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>london england reuters  harry potter star dani...</td>\n",
       "      <td>harry potter star daniel radcliffe gets £20m f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>editors note in our behind the scenes series c...</td>\n",
       "      <td>mentally ill inmates in miami are housed on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  london england reuters  harry potter star dani...   \n",
       "1  editors note in our behind the scenes series c...   \n",
       "\n",
       "                                             summary  \n",
       "0  harry potter star daniel radcliffe gets £20m f...  \n",
       "1  mentally ill inmates in miami are housed on th...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the resulting DataFrames to verify the column names\n",
    "train_df_eng_processed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74913b65-2257-40d4-903e-dcdeef0102e8",
   "metadata": {},
   "source": [
    "## STEP 3: Data size reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b06eb-57a0-48c4-bb3c-79fbb5693ab5",
   "metadata": {},
   "source": [
    "Reducing the size of our training dataset is a crucial step to ensure the efficient use of computational resources and prevent potential crashes during the training process. Given the large size of our datasets—such as the French dataset with nearly `400,000` observations—it is important to balance the need for a representative sample with the limitations of our computing environment. \n",
    "\n",
    "By randomly sampling a subset of `100,000` observations, we maintain the diversity and representativeness of the data while significantly decreasing the computational load. This reduction allows us to streamline the training process, making it more manageable and ensuring smoother execution within the constraints of our Jupyter Notebook environment. This approach is particularly useful at this stage, as it facilitates faster iterations and debugging, ultimately leading to more efficient model development and refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957caa60-6fb6-4639-9e53-a18b2b3879c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 100,000 observations from each training dataset\n",
    "\n",
    "# French dataset\n",
    "train_df_fr_sampled = train_df_fr_processed.sample(n=100000, random_state=42)\n",
    "\n",
    "# German dataset\n",
    "train_df_de_sampled = train_df_de_processed.sample(n=100000, random_state=42)\n",
    "\n",
    "# English dataset\n",
    "train_df_eng_sampled = train_df_eng_processed.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64340e2b-ea47-4f99-9acd-ed39922eb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English dataset (smaller for initial training purposes)\n",
    "train_df_eng_demo = train_df_eng_processed.sample(n=10000, random_state=42)\n",
    "validation_df_eng_demo = validation_df_eng_processed.sample(n=1000, random_state=42)\n",
    "test_df_eng_demo = test_df_eng_processed.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecc994b5-3ba6-49fb-b1ec-4d1f7cbbdba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_eng_demo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586763a-f60c-46b4-a964-86c56d367d84",
   "metadata": {},
   "source": [
    "## STEP 4: Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfaada4c-e957-498e-9a1d-70df1ad98a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of train_df_fr_processed in a new variable train_df_tokenised for all three datasets\n",
    "train_df_fr_tokenised = train_df_fr_sampled.copy()\n",
    "validation_df_fr_tokenised = validation_df_fr_processed.copy()\n",
    "test_df_fr_tokenised = test_df_fr_processed.copy()\n",
    "\n",
    "train_df_de_tokenised = train_df_de_sampled.copy()\n",
    "validation_df_de_tokenised = validation_df_de_processed.copy()\n",
    "test_df_de_tokenised = test_df_de_processed.copy()\n",
    "\n",
    "train_df_eng_tokenised = train_df_eng_sampled.copy()\n",
    "validation_df_eng_tokenised = validation_df_eng_processed.copy()\n",
    "test_df_eng_tokenised = test_df_eng_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7d15f19-a930-4488-be91-2ef7b71d7c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 2)\n",
      "(16059, 2)\n",
      "(15828, 2)\n"
     ]
    }
   ],
   "source": [
    "# Verify by printing the first few rows of the new DataFrame\n",
    "print(train_df_fr_tokenised.shape)\n",
    "print(validation_df_fr_tokenised.shape)\n",
    "print(test_df_fr_tokenised.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d865cf3-a38b-4545-b463-fe216257b9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tokenizer\n",
    "tokenizer = MBart50Tokenizer.from_pretrained('facebook/mbart-large-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "508f8616-9a35-49a6-b7d2-7bed12ea7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenization\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['text'], max_length=700, padding='max_length', truncation=True)\n",
    "    targets = tokenizer(examples['summary'], max_length=150, padding='max_length', truncation=True)\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': targets['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f2bcf7b-b7fc-4b1c-86c0-ac9c5688a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the French dataset\n",
    "train_df_fr_tokenized = train_df_fr_tokenised.apply(preprocess_function, axis=1)\n",
    "validation_df_fr_tokenized = validation_df_fr_tokenised.apply(preprocess_function, axis=1)\n",
    "test_df_fr_tokenized = test_df_fr_tokenised.apply(preprocess_function, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24396bbd-26e1-4e32-8cc7-591e22024b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for German and German datasets\n",
    "train_df_de_tokenized = train_df_de_tokenised.apply(preprocess_function, axis=1)\n",
    "validation_df_de_tokenized = validation_df_de_tokenised.apply(preprocess_function, axis=1)\n",
    "test_df_de_tokenized = test_df_de_tokenised.apply(preprocess_function, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fdb5835-4daf-42e8-a6d7-17a78b969738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 9s, sys: 37.4 s, total: 7min 46s\n",
      "Wall time: 10min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Repeat for German and English datasets\n",
    "train_df_eng_tokenized = train_df_eng_tokenised.apply(preprocess_function, axis=1)\n",
    "validation_df_eng_tokenized = validation_df_eng_tokenised.apply(preprocess_function, axis=1)\n",
    "test_df_eng_tokenized = test_df_eng_tokenised.apply(preprocess_function, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e9e6f84-6e59-4b05-8760-7c885b209353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████| 10000/10000 [00:35<00:00, 282.51 examples/s]\n",
      "Map: 100%|██████████████████████████| 1000/1000 [00:03<00:00, 283.46 examples/s]\n",
      "Map: 100%|██████████████████████████| 1000/1000 [00:03<00:00, 289.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert DataFrame to Dataset format expected by Hugging Face Trainer\n",
    "train_df_eng_hf = Dataset.from_pandas(train_df_eng_demo)\n",
    "validation_df_eng_hf = Dataset.from_pandas(validation_df_eng_demo)\n",
    "test_df_eng_hf = Dataset.from_pandas(test_df_eng_demo)\n",
    "\n",
    "# Apply the preprocessing function to the datasets\n",
    "train_df_eng_demo_tokenized = train_df_eng_hf.map(preprocess_function, batched=True)\n",
    "validation_df_eng_demo_tokenized = validation_df_eng_hf.map(preprocess_function, batched=True)\n",
    "test_df_eng_demo_tokenized = test_df_eng_hf.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146b0fb",
   "metadata": {},
   "source": [
    "# __4. Model Selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa859109-8cad-434e-abcf-da0bdcb77b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 16:50:34.552180: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/rafay/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6b59d2128f4977af35d44df26af04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafay/Documents/Anaconda/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ac79fcadb64bce9b25fb086e5902e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 12:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>6.231728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.423596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.025204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. The model has been saved \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Load the MLSUM dataset for French language\n",
    "dataset_fr = load_dataset(\"mlsum\", \"fr\")\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset_fr['train'])\n",
    "\n",
    "# Shuffle the DataFrame and reset the index\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Select 10 values for training\n",
    "train_df = df.iloc[:10]\n",
    "\n",
    "# Select 2 values for validation\n",
    "validation_df = df.iloc[10:12]\n",
    "\n",
    "# Convert pandas DataFrames to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "\n",
    "# Define the tokenizer and model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples['text']]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['summary'], max_length=150, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine-tuned-bart-mlsum-fr-sampled\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-bart-mlsum-fr-sampled\")\n",
    "\n",
    "print(\"Training complete. The model has been saved \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696d843",
   "metadata": {},
   "source": [
    "# __5. Results Infrence__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284be66c",
   "metadata": {},
   "source": [
    "Since it was not possible to finetune the model we figured out some pretrained models which we used to infrence our results for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193b5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, MarianMTModel, MarianTokenizer\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Seed for reproducibility\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"fine-tuned-bart-mlsum-fr-sampled\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Load translation models and tokenizers\n",
    "translation_models = {\n",
    "    \"fr\": {\n",
    "        \"model\": MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\"),\n",
    "        \"tokenizer\": MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\"),\n",
    "    },\n",
    "    \"de\": {\n",
    "        \"model\": MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\"),\n",
    "        \"tokenizer\": MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, max_length=150, min_length=30):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Function to translate text to English\n",
    "def translate_to_english(text, source_lang):\n",
    "    translation_tokenizer = translation_models[source_lang][\"tokenizer\"]\n",
    "    translation_model = translation_models[source_lang][\"model\"]\n",
    "    \n",
    "    inputs = translation_tokenizer.encode(text, return_tensors=\"pt\", truncation=True)\n",
    "    translated_ids = translation_model.generate(inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "    translated_text = translation_tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translated_text\n",
    "\n",
    "# Function to handle the summarize button click\n",
    "def summarize():\n",
    "    input_text = input_text_area.get(\"1.0\", tk.END).strip()\n",
    "    if not input_text:\n",
    "        output_text_area.delete(\"1.0\", tk.END)\n",
    "        output_text_area.insert(tk.END, \"Please enter text to summarize.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        detected_language = detect(input_text)\n",
    "    except LangDetectException:\n",
    "        output_text_area.delete(\"1.0\", tk.END)\n",
    "        output_text_area.insert(tk.END, \"Language detection failed. Please enter a valid text.\")\n",
    "        return\n",
    "    \n",
    "    language_label_var.set(f\"Detected Language: {detected_language}\")\n",
    "\n",
    "    if detected_language in [\"fr\", \"de\", \"en\"]:\n",
    "        summary = summarize_text(input_text)\n",
    "        if detected_language != \"en\":\n",
    "            summary = translate_to_english(summary, detected_language)\n",
    "    else:\n",
    "        summary = f\"Unsupported language detected: {detected_language}\"\n",
    "\n",
    "    output_text_area.delete(\"1.0\", tk.END)\n",
    "    output_text_area.insert(tk.END, summary)\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"MultiLinguAI\")\n",
    "\n",
    "# Create and place the input text area\n",
    "input_text_label = tk.Label(root, text=\"Input Text:\")\n",
    "input_text_label.pack()\n",
    "input_text_area = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=60, height=10)\n",
    "input_text_area.pack(padx=10, pady=10)\n",
    "\n",
    "# Create and place the summarize button\n",
    "summarize_button = tk.Button(root, text=\"Summarize\", command=summarize)\n",
    "summarize_button.pack(pady=10)\n",
    "\n",
    "# Create and place the language detection label\n",
    "language_label_var = tk.StringVar(value=\"Detected Language: N/A\")\n",
    "language_label = tk.Label(root, textvariable=language_label_var)\n",
    "language_label.pack()\n",
    "\n",
    "# Create and place the output text area\n",
    "output_text_label = tk.Label(root, text=\"Summary:\")\n",
    "output_text_label.pack()\n",
    "output_text_area = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=60, height=10)\n",
    "output_text_area.pack(padx=10, pady=10)\n",
    "\n",
    "# Start the main loop\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93fff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
